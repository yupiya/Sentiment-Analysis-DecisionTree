# utils/tokenization.py

def tokenizes(text):
    
    return text.split()
